# -*- coding: utf-8 -*-
"""cfpb_consumer_complaints_llama_tagging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ucAO_4_Dx7fupbb6rFw00ZXmXJSZZuoA

###ONE TIME TO UNZIP FILE
"""

# Step 1: Mount Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

# Step 2: Unzip directly using shell command
#!unzip -o '/content/drive/MyDrive/data/data/deduplicated_complaints_cleaned.zip' -d '/content/drive/MyDrive/data/data/'

"""###Import libraries"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation #LDA
from google.colab import drive  # Import drive from google.colab

"""### Runtime Check"""

import torch
import time

def check_gpu_and_recommend():
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("No GPU detected! You are running on CPU. RESTART and select GPU in Runtime settings.")
        return

    # Get GPU name
    gpu_name = torch.cuda.get_device_name(0)
    print(f"Detected GPU: {gpu_name}")

    # Decision logic
    gpu_name_lower = gpu_name.lower()

    if "a100" in gpu_name_lower:
        print("Perfect! A100 GPU detected. Proceed with full embedding!")
    elif "l4" in gpu_name_lower:
        print("Great! L4 GPU detected. Proceed — fast and efficient for your workload!")
    elif "t4" in gpu_name_lower:
        print("Good. T4 GPU detected. Proceed, but embedding will be slower.")
    elif "p100" in gpu_name_lower:
        print("Very Good! P100 GPU detected. Proceed — fast embeddings expected.")
    elif "k80" in gpu_name_lower:
        print("Slow GPU (K80) detected. RECOMMEND: Runtime → Restart until you get a T4/L4/A100.")
    else:
        print("Unknown GPU detected. Proceed cautiously.")

    # Additional info
    print("\n Tip: Always save intermediate results if you are not on A100 or L4 to avoid losses!")

# Run the GPU check
check_gpu_and_recommend()

"""### Mount Drive and Load Data"""

from google.colab import drive
drive.mount('/content/drive')

"""#End-to-End Complaint Narrative Clustering and Trend Analysis with External Economic Indicators"

#### Step 1: Install necessary libraries
"""

!pip install -q sentence-transformers hdbscan umap-learn scikit-learn pandas matplotlib pandas_datareader

# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
import hdbscan
import umap
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import pandas_datareader.data as web
import datetime
import torch

from google.colab import drive
drive.mount('/content/drive')

"""#### Step 2: Check if GPU is available"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\033[1mRunning on {device.upper()}\033[0m")

"""#### Step 3: Load filtered dataset from drive & Prepare text data"""

!pip install transformers accelerate torch

# Setup Authentication
from huggingface_hub import login

# Paste your Hugging Face API token here
HUGGINGFACE_TOKEN = "Your API Key"

# Authenticate
login(token=HUGGINGFACE_TOKEN)
print("Hugging Face Authentication Successful!")

#Load the LLaMA model from huggingface
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Choose LLaMA 3.2 model (modify if needed)
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)

# Load model
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.float16, use_auth_token=True)

# Move model to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print("LLaMA 3.2 Model Loaded Successfully!")

import pandas as pd
from tqdm import tqdm


import pandas as pd
import re
from tqdm import tqdm

# Legal Patterns
LEGAL_PATTERNS = [
    re.compile(r"\bFair Credit Reporting Act\b", re.IGNORECASE),
    re.compile(r"\bFair Debt Collection Practices Act\b", re.IGNORECASE),
    re.compile(r"\bFDCPA\b", re.IGNORECASE),
    re.compile(r"\bFCRA\b", re.IGNORECASE),
    re.compile(r"\b15\s*U\.?S\.?C\.?\s*§?\s*\d+[a-zA-Z\-]*\b", re.IGNORECASE),
    re.compile(r"\bSection\s+\d+[a-zA-Z\-]*\s+of\s+the\s+FCRA\b", re.IGNORECASE),
    re.compile(r"\bFlorida state law\b", re.IGNORECASE),
    re.compile(r"\bstatute of limitations\b", re.IGNORECASE),
]

def extract_statute_tags(text):
    matches = set()
    for pattern in LEGAL_PATTERNS:
        matches.update(map(str.strip, pattern.findall(text)))
    return list(matches)

# Buzzword detection
BUZZWORDS = [
    "traffick", "fraud", "identity theft", "credit report error", "unauthorized charges",
    "FCRA violation", "FDCPA violation", "child abuse", "scam", "sexual abuse", "data breach",
    "harassment", "discrimination", "unauthorized access", "debt collection", "overcharged",
    "unlawful practices", "forgery", "abduction", "missing child", "molestation", "foreclosure"
]

buzzword_patterns = {
    word: re.compile(rf"\b{re.escape(word.split()[0])}\w*\b", re.IGNORECASE)
    for word in BUZZWORDS
}

def smart_inject_tags(text, existing_tags):
    lowered_text = text.lower()
    existing_lower = {tag.lower() for tag in existing_tags}
    injected = []

    for base, pattern in buzzword_patterns.items():
        if pattern.search(lowered_text) and base not in existing_lower:
            if base == "traffick":
                injected.append("trafficking")
            elif base in {"child", "sexual", "missing"}:
                continue
            else:
                injected.append(base)

    return list(set(existing_tags + injected))

# Batched tagging function
def generate_tags_batched_safe(
    texts,
    tokenizer,
    model,
    device,
    batch_size=8,
    save_every=10,
    save_path=None
):
    all_tags = []
    num_batches = (len(texts) + batch_size - 1) // batch_size

    for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc="Tagging Complaints (POC-style)"):
        batch = texts[i:i + batch_size]

        prompt = (
            "You are a legal assistant trained to identify key legal and regulatory issues from consumer complaints. "
            "Please generate 2 to 3 concise keyword-style tags that capture the main legal, financial, or risk-related concerns in the complaint. "
            "Avoid hashtags or full sentences.\n\n"
        )
        for j, text in enumerate(batch):
            prompt += f"{j + 1}. {text.strip()}\n"
        prompt += "\nTags:\n"

        try:
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                top_p=1.0,
                temperature=0.0,
                pad_token_id=tokenizer.eos_token_id
            )

            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
            tags_block = decoded.split("Tags:")[-1].strip()
            lines = tags_block.strip().split('\n')

            for j in range(len(batch)):
                try:
                    line = lines[j]
                    match = re.match(r'^\d+\.\s*(.+)', line)
                    if match:
                        cleaned = match.group(1).strip()
                        if 1 <= len(cleaned.split()) <= 6:
                            tags = [tag.strip() for tag in cleaned.split(',') if tag.strip()]
                        else:
                            tags = []
                    else:
                        tags = []

                    tags = smart_inject_tags(batch[j], tags)
                    tags += extract_statute_tags(batch[j])
                    all_tags.append(list(set(tags)))

                except IndexError:
                    all_tags.append([])

        except Exception as e:
            print(f"Batch {i // batch_size + 1} failed: {e}")
            all_tags.extend([[] for _ in batch])

        if save_path and ((i // batch_size + 1) % save_every == 0):
            pd.DataFrame({"Tags": all_tags}).to_csv(save_path, index=False)
            print(f"Saved checkpoint at batch {i // batch_size + 1} to {save_path}")

    return all_tags

import time
from tqdm import tqdm

tqdm.pandas()  # Enables progress bar for pandas apply

drive.mount('/content/drive')

# Load the saved clustered CSV
final_df = pd.read_csv('/content/drive/MyDrive/consumercomplaints/raw/deduplicated_complaints_cleaned.csv')


# Generate tags from each complaint narrative using LLaMA
print("Generating tags in batches...")

texts = final_df['Consumer complaint narrative'].tolist()
tag_results = generate_tags_batched_safe(
    texts,
    tokenizer=tokenizer,
    model=model,
    device=device,
    batch_size=12,   # Try 12 or 16 if your GPU has enough memory
    save_every=10,
    save_path="/content/partial_tags_2.csv"
)

assert len(tag_results) == len(final_df), "Mismatch between tag results and complaints!"
final_df['Tags'] = tag_results

print("Tags generated successfully!")

print("Total rows in final_df:", len(final_df))
print("Tags generated:", len(tag_results))

#Update the final_df by adding additional column to store labels generated by LLaMA. Save it in CS

final_df.to_csv("/content/drive/MyDrive/consumercomplaints/llama_Labels_tags.csv", index=False)