# -*- coding: utf-8 -*-
"""All Trafficking-Trends.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJl8oJMkjKOeazBoSr-oMYaqe9s_JyM8
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation #LDA
from google.colab import drive  # Import drive from google.colab

import torch
import time

def check_gpu_and_recommend():
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("No GPU detected! You are running on CPU. RESTART and select GPU in Runtime settings.")
        return

    # Get GPU name
    gpu_name = torch.cuda.get_device_name(0)
    print(f"Detected GPU: {gpu_name}")

    # Decision logic
    gpu_name_lower = gpu_name.lower()

    if "a100" in gpu_name_lower:
        print("Perfect! A100 GPU detected. Proceed with full embedding!")
    elif "l4" in gpu_name_lower:
        print("Great! L4 GPU detected. Proceed — fast and efficient for your workload!")
    elif "t4" in gpu_name_lower:
        print("Good. T4 GPU detected. Proceed, but embedding will be slower.")
    elif "p100" in gpu_name_lower:
        print("Very Good! P100 GPU detected. Proceed — fast embeddings expected.")
    elif "k80" in gpu_name_lower:
        print("Slow GPU (K80) detected. RECOMMEND: Runtime → Restart until you get a T4/L4/A100.")
    else:
        print("Unknown GPU detected. Proceed cautiously.")

    # Additional info
    print("\n Tip: Always save intermediate results if you are not on A100 or L4 to avoid losses!")

# Run the GPU check
check_gpu_and_recommend()

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

# Load the data
INPUT_CSV = "/content/drive/MyDrive/data/data/rv_approach2_CC_trafficking.csv"
df = pd.read_csv(INPUT_CSV)

# Display basic info
print(df.columns)

print(df['Tags'].head())

# Drop missing tags
df = df.dropna(subset=['Tags'])

# If tags are comma-separated or lists, normalize them
df['tag_list'] = df['Tags'].apply(lambda x: [t.strip().lower() for t in str(x).split(',')])

# Flatten tag list for visualization
all_tags = df['tag_list'].explode()

# Filter trafficking-related tags (optional)
# Example keywords: 'trafficking', 'human trafficking', etc.
trafficking_tags = all_tags[all_tags.str.contains('traffick', na=False)]

# Frequency count
tag_counts = trafficking_tags.value_counts().head(20)

# Bar chart of top trafficking-related tags
plt.figure(figsize=(12, 6))
sns.barplot(x=tag_counts.values, y=tag_counts.index, palette='viridis')
plt.title('Top Trafficking-related Tags')
plt.xlabel('Frequency')
plt.ylabel('Tag')
plt.tight_layout()
plt.show()

# Word Cloud for trafficking-related tags
text = ' '.join(trafficking_tags)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Trafficking Tags')
plt.show()

df['Date received'] = pd.to_datetime(df['Date received'])
df_exploded = df.explode('tag_list')
trafficking_df = df_exploded[df_exploded['tag_list'].str.contains('traffick', na=False)]

# Count over time
time_series = trafficking_df.groupby(pd.Grouper(key='Date received', freq='M'))['tag_list'].count()

# Plot
plt.figure(figsize=(12, 5))
time_series.plot()
plt.title('Monthly Frequency of Trafficking-related Tags')
plt.ylabel('Number of Complaints')
plt.xlabel('Date')
plt.grid(True)
plt.tight_layout()
plt.show()

# Filter trafficking-related complaints
df_exploded = df.explode('tag_list')
trafficking_df = df_exploded[df_exploded['tag_list'].str.contains('traffick', na=False)]

# Group by state (assuming 'state' column exists)
state_counts = trafficking_df['State'].value_counts().head(20)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x=state_counts.values, y=state_counts.index, palette='rocket')
plt.title('Top 20 States by Trafficking-related Complaints')
plt.xlabel('Number of Complaints')
plt.ylabel('State')
plt.tight_layout()
plt.show()

# Filter trafficking-related complaints
df_exploded = df.explode('tag_list')
trafficking_df = df_exploded[df_exploded['tag_list'].str.contains('traffick', na=False)]

# Group by state (assuming 'state' column exists)
state_counts = trafficking_df['ZIP code'].value_counts().head(20)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x=state_counts.values, y=state_counts.index, palette='rocket')
plt.title('Top 20 States by Trafficking-related Complaints')
plt.xlabel('Number of Complaints')
plt.ylabel('ZIP code')
plt.tight_layout()
plt.show()

# Load ZIP-to-City mapping (adjust filename/path)
zip_city_df = pd.read_csv('uszips.csv')

# Ensure ZIP codes are strings and align format
zip_city_df['zip'] = zip_city_df['zip'].astype(str).str.zfill(5)
trafficking_df['ZIP code'] = trafficking_df['ZIP code'].astype(str).str.zfill(5)

# Merge trafficking complaints with city info
trafficking_with_city = trafficking_df.merge(zip_city_df[['zip', 'city', 'state_id']],
                                             left_on='ZIP code', right_on='zip', how='left')

# Count complaints by city
city_counts = trafficking_with_city['city'].value_counts().head(20)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x=city_counts.values, y=city_counts.index, palette='mako')
plt.title('Top 20 Cities by Trafficking-related Complaints')
plt.xlabel('Number of Complaints')
plt.ylabel('City')
plt.tight_layout()
plt.show()

# Combine city and state for labeling
trafficking_with_city['city_state'] = trafficking_with_city['city'] + ', ' + trafficking_with_city['state_id']

# Group by city_state
city_counts = trafficking_with_city['city_state'].value_counts().head(20)

# Plot
plt.figure(figsize=(14, 7))
sns.barplot(x=city_counts.values, y=city_counts.index, palette='mako')
plt.title('Top 20 Cities by Trafficking-related Complaints')
plt.xlabel('Number of Complaints')
plt.ylabel('City, State')
plt.tight_layout()
plt.show()

# Ensure date column is datetime
trafficking_with_city['Date received'] = pd.to_datetime(trafficking_with_city['Date received'])

# Create month label
trafficking_with_city['month'] = trafficking_with_city['Date received'].dt.to_period('M').astype(str)

# Combine city and state
trafficking_with_city['city_state'] = trafficking_with_city['city'] + ', ' + trafficking_with_city['state_id']

# Filter top N cities overall
top_cities = trafficking_with_city['city_state'].value_counts().head(20).index

# Filter data to top cities
filtered_df = trafficking_with_city[trafficking_with_city['city_state'].isin(top_cities)]

# Create pivot table: rows = city_state, columns = month, values = complaint count
pivot_table = filtered_df.pivot_table(index='city_state', columns='month',
                                      values='tag_list', aggfunc='count', fill_value=0)

# Plot heatmap
plt.figure(figsize=(14, 8))
sns.heatmap(pivot_table, annot=True, fmt='d', cmap='YlOrRd')
plt.title('Monthly Trafficking-related Complaints by City, State')
plt.xlabel('Month')
plt.ylabel('City, State')
plt.tight_layout()
plt.show()

# Group by month and city_state
monthly_city_counts = filtered_df.groupby(['month', 'city_state']).size().unstack(fill_value=0)

# Plot stacked bar
monthly_city_counts.plot(kind='bar', stacked=True, figsize=(16, 7), colormap='tab20')
plt.title('Monthly Distribution of Trafficking-related Complaints by Top Cities')
plt.xlabel('Month')
plt.ylabel('Number of Complaints')
plt.legend(title='City, State', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import plotly.express as px
import pandas as pd

# Prepare the data
grouped = filtered_df.groupby(['month', 'city_state']).size().reset_index(name='count')

# Create interactive bar chart
fig = px.bar(grouped,
             x='month',
             y='count',
             color='city_state',
             title='Monthly Distribution of Trafficking-related Complaints by City, State',
             labels={'count': 'Number of Complaints', 'city_state': 'City, State'},
             hover_data={'month': True, 'city_state': True, 'count': True})

# Improve layout
fig.update_layout(barmode='stack', xaxis_title='Month', yaxis_title='Number of Complaints')

# Show the chart
fig.show()

# Ensure date format
trafficking_with_city['Date received'] = pd.to_datetime(trafficking_with_city['Date received'])
trafficking_with_city['month'] = trafficking_with_city['Date received'].dt.to_period('M').astype(str)

# Filter relevant rows
df_map = trafficking_with_city[trafficking_with_city['tag_list'].astype(str).str.contains('traffick', case=False)]

# Aggregate
map_data = df_map.groupby(['month', 'state_id']).size().reset_index(name='count')

import plotly.express as px

fig = px.choropleth(
    map_data,
    locations='state_id',
    locationmode='USA-states',
    color='count',
    scope='usa',
    animation_frame='month',
    color_continuous_scale='Reds',
    title='Monthly Trafficking-related Complaints by U.S. State',
    labels={'count': 'Number of Complaints'}
)

fig.update_layout(
    geo=dict(bgcolor='rgba(0,0,0,0)'),
    title_x=0.5,
    coloraxis_colorbar=dict(title="Complaints"),
    margin=dict(l=0, r=0, t=40, b=0)
)

fig.show()

# Mapping of state codes to full names
state_name_map = {
    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',
    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',
    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',
    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',
    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',
    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',
    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',
    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',
    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee',
    'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',
    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia'
}

# Add full state names for hover
map_data['state_name'] = map_data['state_id'].map(state_name_map)

# Rebuild interactive map with better hover
fig = px.choropleth(
    map_data,
    locations='state_id',
    locationmode='USA-states',
    color='count',
    scope='usa',
    animation_frame='month',
    hover_name='state_name',  # <- This is what displays full name
    hover_data={'state_id': True, 'count': True, 'month': True},
    color_continuous_scale='Reds',
    title='Monthly Trafficking-related Complaints by U.S. State',
    labels={'count': 'Number of Complaints'}
)

fig.update_layout(
    geo=dict(bgcolor='rgba(0,0,0,0)'),
    title_x=0.5,
    coloraxis_colorbar=dict(title="Complaints"),
    margin=dict(l=0, r=0, t=40, b=0)
)

fig.show()

map_data.to_csv("trafficking_summary_by_month.csv", index=False)

pip install -U langchain-community

pip install faiss-cpu

!pip install -U langchain-community faiss-cpu

pip install -U langchain-openai

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

# Set your OpenAI key securely
os.environ["OPENAI_API_KEY"] = "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"

# Load text file
loader = TextLoader("us_trafficking_laws.txt")
docs = loader.load()

# Split text into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(docs)

# Embed and store in vector DB
embedding = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(split_docs, embedding)

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# Step 1: Define retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# Step 2: Set up the language model
llm = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=os.environ["OPENAI_API_KEY"])

# Step 3: Build the RAG chain
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

# Step 4: Run a query based on structured trafficking data
query = """
Based on recent spikes in human trafficking complaints in Texas and Florida,
what legal actions or task force responses are recommended under the TVPA or U.S. Code?
"""

result = rag_chain.run(query)
print(result)

import pandas as pd
from langchain.schema import Document

# Load the exported CSV
df = pd.read_csv("trafficking_summary_by_month.csv")

# Convert each row into a textual summary
summary_docs = []
for _, row in df.iterrows():
    sentence = (
        f"In {row['month']}, the state of {row['state_name']} ({row['state_id']}) "
        f"reported {row['count']} trafficking-related complaints."
    )
    doc = Document(page_content=sentence, metadata={"source": "complaint_summary"})
    summary_docs.append(doc)

# Add to FAISS vectorstore
vectorstore.add_documents(summary_docs)

query = """
Which states saw the most trafficking-related complaints in July 2024, and what legal actions are supported by the TVPA for high-incidence regions?
"""
result = rag_chain.run(query)
print(result)

query = """
I need a well-organized summary in bullet points.

Context:
- Complaint data shows that Virginia had the highest number of trafficking-related complaints in July 2024 (3 reports), followed by Tennessee (1 report).
- We're looking to understand the legal actions supported by the Trafficking Victims Protection Act (TVPA) that apply to high-complaint regions.

Please respond using this structure:

📊 Key Insights – July 2024 Trafficking Complaints
• ...
• ...

⚖️ Legal Actions under the TVPA
• ...
• ...

💡 Tips for Analysts or Policymakers
• ...

Be concise but informative. Use bullet points and emoji section headers as shown.
"""

result = rag_chain.run(query)
print(result)