# -*- coding: utf-8 -*-
"""cfpb_consumer_complaints_llama_theme.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUs2oqsRbIR38CXbvb5Y9Ko6sGIJO1wa

###ONE TIME TO UNZIP FILE
"""

# Step 1: Mount Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

# Step 2: Unzip directly using shell command
#!unzip -o '/content/drive/MyDrive/data/data/deduplicated_complaints_cleaned.zip' -d '/content/drive/MyDrive/data/data/'

"""###Import libraries"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation #LDA
from google.colab import drive  # Import drive from google.colab

"""### Runtime Check"""

import torch
import time

def check_gpu_and_recommend():
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("No GPU detected! You are running on CPU. RESTART and select GPU in Runtime settings.")
        return

    # Get GPU name
    gpu_name = torch.cuda.get_device_name(0)
    print(f"Detected GPU: {gpu_name}")

    # Decision logic
    gpu_name_lower = gpu_name.lower()

    if "a100" in gpu_name_lower:
        print("Perfect! A100 GPU detected. Proceed with full embedding!")
    elif "l4" in gpu_name_lower:
        print("Great! L4 GPU detected. Proceed ‚Äî fast and efficient for your workload!")
    elif "t4" in gpu_name_lower:
        print("Good. T4 GPU detected. Proceed, but embedding will be slower.")
    elif "p100" in gpu_name_lower:
        print("Very Good! P100 GPU detected. Proceed ‚Äî fast embeddings expected.")
    elif "k80" in gpu_name_lower:
        print("Slow GPU (K80) detected. RECOMMEND: Runtime ‚Üí Restart until you get a T4/L4/A100.")
    else:
        print("Unknown GPU detected. Proceed cautiously.")

    # Additional info
    print("\n Tip: Always save intermediate results if you are not on A100 or L4 to avoid losses!")

# Run the GPU check
check_gpu_and_recommend()

"""### Mount Drive and Load Data"""

from google.colab import drive
drive.mount('/content/drive')

"""#End-to-End Complaint Narrative Clustering and Trend Analysis with External Economic Indicators"

#### Step 1: Install necessary libraries
"""

!pip install -q sentence-transformers hdbscan umap-learn scikit-learn pandas matplotlib pandas_datareader

# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
import hdbscan
import umap
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import pandas_datareader.data as web
import datetime
import torch

from google.colab import drive
drive.mount('/content/drive')

"""#### Step 2: Check if GPU is available"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\033[1mRunning on {device.upper()}\033[0m")

"""#### Step 3: Load filtered dataset from drive & Prepare text data"""

!pip install transformers accelerate torch

# Setup Authentication
from huggingface_hub import login

# Paste your Hugging Face API token here
HUGGINGFACE_TOKEN = "Your API Key"

# Authenticate
login(token=HUGGINGFACE_TOKEN)
print("Hugging Face Authentication Successful!")

#Load the LLaMA model from huggingface
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Choose LLaMA 3.2 model (modify if needed)
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)

# Load model
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.float16, use_auth_token=True)

# Move model to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print("LLaMA 3.2 Model Loaded Successfully!")

import re

def clean_theme_output(theme_text):
    if not isinstance(theme_text, str):
        return ""

    cleaned = re.sub(r"[*_`]+", "", theme_text)
    cleaned = re.split(r'[.:\n]', cleaned)[0].strip()
    cleaned = re.sub(r"\s+", " ", cleaned)

    for bad_start in [
        "you are an expert", "complaint", "note", "please", "let me know",
        "theme label", "respond"
    ]:
        if cleaned.lower().startswith(bad_start):
            return ""

    words = cleaned.split()
    if len(words) > 10:
        cleaned = " ".join(words[:7])
    return cleaned.strip()

BAD_THEMES = {"", "complaint", "dispute", "issue", "problem", "concern"}

def final_theme_filter(theme):
    return theme if theme.lower().strip() not in BAD_THEMES else ""

# -------------------------
# THEME GENERATION
# -------------------------
def generate_single_theme(complaint_text, tokenizer, model, device, max_retries=3):
    prompt_template = (
        "You are an expert analyst. Read the consumer complaint below and generate a short, concise theme label (not a full sentence). "
        "Your output must be 3 to 7 words only, like a title. Do not explain anything. "
        "Examples: Identity Theft, FDCPA Violation, Inaccurate Credit Report, Unauthorized Credit Pull.\n\n"
        "Complaint:\n{}\n\nTheme Label:"
    )

    for attempt in range(max_retries):
        try:
            prompt = prompt_template.format(complaint_text.strip())
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
            theme = decoded.split("Theme Label:")[-1].strip()
            cleaned = clean_theme_output(theme)
            if cleaned:
                return cleaned
        except Exception:
            continue
    return ""

# -----------------------------
# Main Execution Starts Here
# -----------------------------

# Load model and tokenizer
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.float16, use_auth_token=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"LLaMA model loaded on {device}!")

import os
import re
import pandas as pd
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# CONFIGURATION
HUGGINGFACE_TOKEN = "Your API Key"  # üîÅ Replace with your actual token
MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
INPUT_CSV = "/content/drive/MyDrive/consumercomplaints/raw/deduplicated_complaints_cleaned.csv"
CHECKPOINT_CSV = "/content/drive/MyDrive/consumercomplaints/partial_theme_checkpoint_llama.csv"
FINAL_OUTPUT_CSV = "/content/drive/MyDrive/consumercomplaints/llama_Labels_theme.csv"

BATCH_SIZE = 16

# AUTHENTICATION & MODEL LOADING
login(token=HUGGINGFACE_TOKEN)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.float16, use_auth_token=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# THEME CLEANER
def clean_theme_output(theme_text):
    if not isinstance(theme_text, str):
        return ""
    cleaned = re.sub(r"[*_`]+", "", theme_text)
    cleaned = re.split(r'[.:\n]', cleaned)[0].strip()
    cleaned = re.sub(r"\s+", " ", cleaned)
    words = cleaned.split()
    return cleaned.strip() if 2 <= len(words) <= 10 else ""

# SINGLE THEME GENERATOR
def generate_single_theme(text, tokenizer, model, device, max_retries=3):
    prompt = (
        "You are an expert analyst. Read the consumer complaint below and generate a short, concise theme label (not a full sentence). "
        "Your output must be 3 to 7 words only, like a title. Do not explain anything. "
        "Examples: Identity Theft, FDCPA Violation, Inaccurate Credit Report, Unauthorized Credit Pull.\n\n"
        f"Complaint:\n{text.strip()}\n\nTheme Label:"
    )

    def try_once(sample=True):
        try:
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                do_sample=sample,
                top_p=0.9 if sample else None,
                temperature=0.7 if sample else None,
                pad_token_id=tokenizer.eos_token_id
            )
            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
            result = decoded.split("Theme Label:")[-1].strip()
            return clean_theme_output(result)
        except:
            return ""

    for _ in range(max_retries):
        theme = try_once(sample=True)
        if theme:
            return theme
    return try_once(sample=False)

# LOAD INPUT
df = pd.read_csv(INPUT_CSV)
df['Consumer complaint narrative'] = df['Consumer complaint narrative'].fillna("")

# LOAD PROGRESS IF EXISTS
if os.path.exists(CHECKPOINT_CSV):
    checkpoint_df = pd.read_csv(CHECKPOINT_CSV, index_col=0)
    if "Themes" not in checkpoint_df.columns:
        checkpoint_df["Themes"] = ""
    done_indices = set(checkpoint_df.index)
    print(f"Resuming from checkpoint: {len(done_indices)} records already processed.")
else:
    checkpoint_df = pd.DataFrame(columns=["Themes"])
    done_indices = set()

# PROCESS IN BATCHES
for i in tqdm(range(len(df))):
    if i in done_indices:
        continue

    text = df.at[i, "Consumer complaint narrative"]
    theme = generate_single_theme(text, tokenizer, model, device)

    checkpoint_df.loc[i] = [theme]

    if i % BATCH_SIZE == 0:
        checkpoint_df.to_csv(CHECKPOINT_CSV)
        print(f"Checkpoint saved at index {i}")

# MERGE FINAL OUTPUT
df["Themes"] = checkpoint_df["Themes"]
df.to_csv(FINAL_OUTPUT_CSV, index=False)
print(f"\n‚úÖ Final results saved to: {FINAL_OUTPUT_CSV}")